= Network Configuration Manager Design
Eric Li <sze.li@futurewei.com>, Liguang Xie <lxie@futurewei.com>, Chun-Jen (James) Chung<cchung@futurewei.com>
v0.1, 2021-01-28
:toc: right
:sectnums:
:imagesdir: ../../images

NOTE: This document is under development

== Architecture Overview

image:NCM_overview.png[image,width=880,height=640]

== Requirements

We want to have a design that is:

. Ultra high scale - supports up to a million compute hosts in a region
. Efficient control messaging - no chatty protocol like neutron OVS agent
. Super fast dataplane provisioning for both small and large VPC
. Small ACA on host - low resource, simple, can be ran on smart NIC's ARM CPU

== Problems to Solve

Other than meeting the requirements above, there are a list of problems to be solved with this design:

. Neighbor and Security Group scale - address scaling issue especially with remote security group rule
. Duplicate configuration - don't send down redundant configuration when compute node has it already
. Out of Order configuration - detect and resolve out of order configuration in goal state messages
. Alcor Control Agent restart/upgrade handling - provide an efficient way to restore the configuration
. Direct Dataplane programming - reduce the logic and local data needed in ACA

== Network Configuration Manager

Network Configuration Manager is introduced here to meet the ultra high scale requirement for the next generation cloud. A set of nearby compute nodes (in networking term, like their TOR switch is connected to adjacent physical switchs) will be grouped together to form an cluster managed by a set of NCMs. There could be 10,000 machines serviced by this set of NCMs if it meets the performance requirement.

The set of NCMs will store both the latest full state and potential delta states for a resource belongs to a particular compute node, together with the corresponding version numbers. The version numbers are used for out of order detection and handling discuss later. The database will track whether a resource has been sent down to ACA or not. Since the database will be shared with multiple instances of NCMs, lock is needed when reading/writing to a particular resource belongs to a compute host. It is a small and localized lock therefore it should not impact performance. 

NCM will serve as configuration cache or passthrough proxy for network configurations. For port/router/gateway, NCM will always pass down the configuration because the scale is bounded. For neighbor and security group, a hybrid model will be used based on VPC size. That is to pass down to ACA for small VPC, and don't pass down to ACA for large VPC. 

== Database Requirements

In order to support the Network Configuration Manager, we have the following database requirements:

. Persistent - old data should always be there and not flushed
. Distributed - multiple instances of NCMs can access it concurrently
. Performance - less than 1 millisecond for needed data (e.g. 1000 request per second, each request has multiple reads on around 10 tables with 100gig of configuration stored in database)

== Where to Implement NCM?

There are two options in where NCM could be implemented. Prefering option 1 for the initial POC implementation.

=== Option 1: Make it part of DPM in region level

[source,shell]
------------------------------------------------------------
   +------------------------+
   | +------------------------+
   | | +------------------------+         +--------------------+
   | | |                    | | |         |                    |
   | | |   DPM with NCM     | | |         |                    |
   | | |   functionality    | | |         |                    |
   | | |                    | | +-------->+      ACA           |
   | | |                    | | |         |                    |
   | | |                    | | |         |                    |
   | | |                    | | |         |                    |
   +------------------------+ | |         +--------------------+
     +------------------------+ +--------+
       +------------------------+        |
                   |                     |
                   |                     |
         +---------v----------+          >---------------------+
         |                    |           |                    |
         |                    |           |                    |
         |    ACA             |           |     ACA            |
         |                    |           |                    |
         |                    |           |                    |
         |                    |           |                    |
         +--------------------+           +--------------------+
------------------------------------------------------------

Since all the network configurations are passed down by DPM placed at the region level, it is possible to integration all the NCM functionalities into DPM and reduce the extra layer and component of NCM. However, since DPM is placed at the region level, the on demand requests from ACA may need to take a few extra hops to DPM which can introduce additional latency for the time critical on demand requests.

=== (preferred) Option 2: Separate NCM placed in the cluster level

[source,shell]
------------------------------------------------------------
+------------------------+                +------------------------+
| +------------------------+              | +------------------------+
| | +------------------------+            | | +------------------------+         +--------------------+
| | |                    | | |            | | |                    | | |         |                    |
| | |   DPM              +--------------->+ | |   NCM              | | |         |                    |
| | |                    | | |            | | |                    | | |         |                    |
| | |                    | +------------->+ | |                    | | +-------->+      ACA           |
| | |                    | | |            | | |                    | | |         |                    |
| | |                    | | +----------->+ | |                    | | |         |                    |
| | |                    | | |            | | |                    | | |         |                    |
+------------------------+ | |            +------------------------+ | |         +--------------------+
  +------------------------+ |              +------------------------+ +--------+
    +------------------------+                +------------------------+        |
                                                          |                     |
                                                          |                     |
                                                +---------v----------+          >+--------------------+
                                                |                    |           |                    |
                                                |                    |           |                    |
                                                |    ACA             |           |     ACA            |
                                                |                    |           |                    |
                                                |                    |           |                    |
                                                |                    |           |                    |
                                                +--------------------+           +--------------------+
------------------------------------------------------------

To meet the time critical on demand requests, NCM can be a separate component placed at the cluster level. Since NCM is in close network proximity with its ACA compute nodes, network latency should be lower. This model also partition the database to store only the clustered ACA compute nodes. This is currently the preferred option.

== Compute/ACA Node Metadata

Each NCM needs to know the list of compute/ACA nodes under it. The Node Metadata manager will push down that information to the corresponding NCM during compute/ACA nodes bring up. In the error situation when there is a goal state message targetting to a compute/ACA node that's not known to NCM, NCM will query Node Metadata manager to get the information directly.

== gRPC Connection change

We will update an existing gRPC interface for the bidirectional streaming connection between ACA and NCM:

[source,shell]
------------------------------------------------------------
from: 
    rpc PushNetworkResourceStatesStream (stream GoalState) returns (stream GoalStateOperationReply)

to: 
    rpc PushNetworkResourceStatesStream (stream GoalState) returns (stream GoalStateStatus)
------------------------------------------------------------

NCM Workflow for the gRPC connection from the client side:

. Setup gRPC client connection: GoalStateProvisionerClient grpc_client(grpc::CreateChannel(
        g_grpc_server + ":" + g_grpc_port, grpc::InsecureChannelCredentials()));
. Create the bi-directional stream: std::shared_ptr<ClientReaderWriter<GoalState, GoalStateOperationReply> > stream(
        stub_->PushNetworkResourceStatesStream(&context));
. Create a new thread for the writer: std::thread writer([stream, ...]() {...} 
    .. write and send a dummy no-op message to ACA
    .. wait for other message to be sent to ACA
    .. don't close this writing thread as the server side will shutdown the connection
. In the original thread, have a while loop to keep reading for GoalStateStatus
. Join the writer thread: writer.join();
. Cleanup since both reader and writer are done if we reach here

For reference, the above is already implemented in ACA test code: https://github.com/futurewei-cloud/alcor-control-agent/blob/master/test/func_tests/gs_tests.cpp#L289 

Note: may need to send stuff periodically to keep the gRPC connection alive.

== Security Group Improvements

Question: who is going to resolve the SG remote IPs before sending down the ACA? Answer: Likely SGM
Another option is to couple neighbor configure with SG by added the assoicated SG? But that won't work with cross VPC remote SG group

Security group handling is one of the biggest challenge for public cloud due to its scaling issue especially with remote SG group assoication in a rule. E.g. we have an ingress rule to allow ingress traffic only from the ports assoicated with a default SG. As ports assoication comes and goes, all the existing ports needs to know the latest set of port IPs assoicated with this default SG with the current openstack neutron solution today.  

One approach to address the SG scale issue is to mark each packet with source port SG ID/label. Instead of knowing all the remote IPs on an ingress SG remote rule on the destination side, we can simply mark all egress packets with its associated SG IDs/labels. On the ingress side, it only needs match the ingress remote rule SG ID/label with the marking in the packet. Note that this will greatly help with the scale and IP updates for the ingress remote rule only, but it is an elegent solution which addresses half of the problem for SG. 

One challenge is the current SG ID is a 16 bytes UUID, and each port can be assoicated with upto 5 SG IDs. With the overhead of NSH header or IP options approach, we are looking at adding close to 100 bytes to each packet (16bytesx5=80bytes + overhead). SG ID labeling can be used to reduce adding so much data per packet. Alcor security group manager can generate SG ID label per VPC (or per tenant) and passes it down together with its SG configuration to ACA. Since there is a limit for how many SGs a tenant can create (e.g. 50 per tenant), 1 byte with 256 values should be big enough for the SG ID label.

== ACA on demand request of configuration

There may be situations when ACA doesn't have the needed configurations for a new packet inflight. When that happens, the packet will be punt to ACA and ACA will request info from NCM using PushNetworkResourceStatesStream mentioned above.

Input from ACA to NCM: GoalStateRequest - request_type=ON_DEMAND, request_id, tunnel_id, source port IP, destination IP, source/destination port, protocol - TCP/UDP/Other(ARP/ICMP) 

NCM Workflow:

. Find the source port ID based on IP using tunnel ID to lookup VPC
.. For destination IP on the same subnet, confirm it is L2 neighbor
.. For destination IP on the different subnet, confirm it is L3 neighbor
. Once confirm it is L2/L3 neighbor, look up SG rules for source port
.. If traffic is allowed, construct and track the corresponding SG config
... send down neighbor and corresponding constructed SG rule (first step)
... send down port configuration with Operation = INFO (routable) with corresponding request ID (second step)
... May go ahead to send down remaining neighbor and SG config for this active port
.. If traffic is not allowed
... send down port configuration with Operation = NOT_ROUTABLE (?) with corresponding request ID

== Schema Update

=== New GRPC stream communication

*src/schema/proto3/common.proto*
[source,java]
------------------------------------------------------------
enum OperationType {
    INFO = 0;
    CREATE = 1;
    UPDATE = 2;
    GET = 3;
    DELETE = 4;
    ROUTABLE = 7;     // NEW
    NOT_ROUTABLE = 6; // NEW
}

enum OperationStatus {
    SUCCESS = 0;
    FAILURE = 1;
    INVALID_ARG = 2;
    PENDING = 3;
    ON_DEMAND = 4;
    OUT_OF_ORDER = 5;
    RESTARTED = 6;
}

message HostInfo { // NEW - moved from port.proto
   string ip_address = 1;
   string mac_address = 2;
   }
------------------------------------------------------------

*src/schema/proto3/goalstateprovisioner.proto*
[source,java]
------------------------------------------------------------
message GoalStateOperationReply {
    
    message GoalStateOperationStatus {
        string resource_id = 1;
        ResourceType resource_type = 2;
        OperationType operation_type = 3;
        OperationStatus operation_status = 4;
        uint32 dataplane_programming_time = 5;
        uint32 network_configuration_time = 6;
        uint32 state_elapse_time = 7;
    }

    repeated GoalStateOperationStatus operation_statuses = 1;

    // Total operation time (in nanoseconds)
    //    1. to process the message (consisting of multiple operations)
    //    2. to program data plane
    // Note: The list of operation_statuses details the time spent at each operation
    uint32 message_total_operation_time = 2;
}

message GoalStateRequest {

    message ResourceStateRequest {
        OperationStatus operation_status = 1;
        string request_id = 2; // UUID generated by ACA
        uint32 tunnel_id = 3; 
        ResourceType resource_type = 4; // should be PORT
        string resource_id = 5;
        string source_ip = 6; // use either port resource ID or IP
        uint32 source_port = 7;
        string destination_ip = 8; 
        uint32 destination_port = 9;
        EtherType ethertype = 10;
        Protocol protocol = 11;
    }

    repeated ResourceStateRequest state_requests = 1;
}

message GoalStateStatus {
    uint32 format_version = 1;

    repeated GoalStateOperationReply gs_operation_replies = 2;
    repeated GoalStateRequest gs_requests = 3;
}
------------------------------------------------------------

*src/schema/proto3/vpc.proto*
[source,java]
------------------------------------------------------------
enum VpcSize { // *** NEW ***
    DEFAULT = 0;
    SMALL = 1;
    CHANGING_TO_LARGE = 2;  // *** DO WE NEED THIS?
    LARGE = 3;
    CHANGING_TO_SMALL = 4;
}

message VpcConfiguration {  
    uint32 revision_number = 1; // resource manager needs to fill in

    string request_id = 2;
    string id = 3;
    UpdateType update_type = 4; // DELTA (default) or FULL *** REMOVE THIS? ***
    VpcSize vpc_size = 5; // *** NEW ***
    string project_id = 6;
    string name = 7;
    string cidr = 8;
    uint32 tunnel_id = 9;

    repeated string gateway_ids = 10;
}

message VpcState {
    OperationType operation_type = 1;
    VpcConfiguration configuration = 2;
}
------------------------------------------------------------

=== Putting in targetted host info into GS message

First and suggested option is:

*alcor/src/schema/proto3/goalstate.proto*

[source,java]
------------------------------------------------------------
...
message GoalState {
    uint32 format_version = 1;

    map<string /*host ip-ResourceType enum*/, string /*resource id*/> host_to_resource_id = 2;
    map<string /*resource id*/, VpcState> vpc_states = 3;
    map<string /*resource id*/, SubnetState> subnet_states = 4;

    // PortState and DHCPState is applicable to one host only
    map<string /*resource id*/, PortState> port_states = 5;
    map<string /*resource id*/, DHCPState> dhcp_states = 6;

    map<string /*resource id*/, NeighborState> neighbor_states = 7;
    map<string /*resource id*/, SecurityGroupState> security_group_states = 8;
    map<string /*resource id*/, RouterState> router_states = 9;
    map<string /*resource id*/, GatewayState> gateway_states = 10;
}
------------------------------------------------------------

Second option is:

*alcor/src/schema/proto3/goalstate.proto*

[source,java]
------------------------------------------------------------
...
message GoalState {
    uint32 format_version = 1;

    map<string /*host id*/, string /*vpc id*/> host_to_vpc_id = 2;
    map<string /*resource id*/, VpcState> vpc_states = 3;

    map<string /*host id*/, string /*subnet id*/> host_to_subnet_id = 3;
    map<string /*resource id*/, SubnetState> subnet_states = 4;

    // PortState and DHCPState is applicable to one host only
    map<string /*resource id*/, PortState> port_states = 5;
    map<string /*resource id*/, DHCPState> dhcp_states = 6;

    map<string /*host id*/, string /*neighbor id*/> host_to_neighbor_id = 2;
    map<string /*resource id*/, NeighborState> neighbor_states = 3;

    map<string /*host id*/, sting /*security group id*/> host_to_security_group_id = 2;
    map<string /*resource id*/, SecurityGroupState> security_group_states = 3;

    map<string /*host id*/, sting /*router id*/> host_to_router_id = 2;
    map<string /*resource id*/, RouterState> router_states = 3;

    map<string /*host id*/, sting /*gateway id*/> host_to_gateway_id = 2;
    map<string /*resource id*/, GatewayState> gateway_states = 3;
}
------------------------------------------------------------

Third option is to put targeted_host_info into vpc/subnet/neighbor/SG/router/Gateway state:

*alcor/src/schema/proto3/neighbor.proto*

[source,java]
------------------------------------------------------------
message NeighborState {
    repeated HostInfo targeted_host_info = 1; // NEW, host of ACA host IP

    OperationType operation_type = 2;
    NeighborConfiguration configuration = 3;
}
------------------------------------------------------------

== NCM Out of Order Configuration Handling

Detection - all resource managers needs to fill in revision_number for a given resource (e.g. Port/Neighbor etc). It should have not problem to generate the revision_number since it already has a lock when dealing with a particular resource. Both NCM and ACA can detect out of order configuration, but it is higher priority to do it in NCM.

NCM already keep tracks of all resources for a particular compute host, together with its revision_number. When NCM detect there is an out of order configuration for a particular resource, NCM should respond to DPM using GoalStateOperationReply message and mark a resource's operation_status = OUT_OF_ORDER.

== ACA Restart Handling

See issue #540, ACA restart handling is described below:

=== Neutron OVS Agent Behavior

Neutron OVS agent inserts a canary table during startup. In its main rpc_loop, it will always check on the ovs status by querying the canary table. ovs_status will be set of OVS_RESTARTED if the canary table is not found. 

To handle the OVS_RESTARTED situation, it will re-setup the bridges (br-int, br-tun, etc) and default flows. It will also reset the dvr if it is enabled. After that, it will rely on a background syncing to get the latest tunnels (for L2 neighbors) and DVR (for L3 neighbors) configurations.

=== Network Configuration Manager Solution

With Network Configuration Manager acting as configuration cache for each compute host. When ACA has detected the dataplane (e.g. OVS) has been restarted, ACA will send GoalStateRequest to NCM with request_type=RESTARTED. This signals NCM that a partcular ACA needs its help to bring down all the configurations. 

Input from ACA to NCM: GoalStateRequest - request_type=RESTARTED, request_id (generated by ACA)

NCM will use existing algorthm to bring down all the configuration for ports/routers/gateways (small or big VPC), and neighbor + security group configuration according to VPC size.

== Work Flows

image:NCM_workflow.png[image,width=880,height=640]

== Highlevel Slides

Please find the highlevel powerpoint slides of Network Configuration Manager (NCM) in xref:NCM_design.pptx[Network Configuration Manager]

[bibliography]
== References

- [[[proto3-map,1]]] https://developers.google.com/protocol-buffers/docs/proto3#maps
- [[[map.h-java,2]]] https://developers.google.com/protocol-buffers/docs/reference/java-generated#map-fields
- [[[map.h,3]]] https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.map